{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ukmA7z-dlB6m"
      },
      "source": [
        "# TensorFlow!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6S7QNZqwmBOP"
      },
      "outputs": [],
      "source": [
        "import tensorflow\n",
        "import tensorflow.keras.models as models\n",
        "import tensorflow.keras.layers as layers\n",
        "import tensorflow.keras.utils as utils\n",
        "import tensorflow.keras.optimizers as optimizers\n",
        "\n",
        "client1_model = tensorflow.keras.models.load_model(\n",
        "    'model_weight/global_model.h5')\n",
        "client2_model = tensorflow.keras.models.load_model(\n",
        "    'model_weight/global_model.h5')\n",
        "client3_model = tensorflow.keras.models.load_model(\n",
        "    'model_weight/global_model.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ck79-w2ZxwVB"
      },
      "source": [
        "# It's training time!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "both",
        "id": "CkOXxmoVyHdc"
      },
      "outputs": [],
      "source": [
        "import numpy\n",
        "import tensorflow.keras.callbacks as callbacks\n",
        "\n",
        "\n",
        "def get_dataset():\n",
        "\tcontainer = numpy.load('dataset.npz') # Download dataset.npz from the link in Readme.md\n",
        "\tb, v = container['b'], container['v']\n",
        "\tv = numpy.asarray(v / abs(v).max() / 2 + 0.5, dtype=numpy.float32) # normalization (0 - 1)\n",
        "\treturn b, v\n",
        "\n",
        "x_train, y_train = get_dataset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split dataset for two clients\n",
        "# pip install -U scikit-learn\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "client1_x, client2_x, client1_y, client2y = train_test_split(x_train, y_train, test_size=0.33, random_state=100)\n",
        "len(client1_x)\n",
        "len(client2_x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Dellocate variable to save RAM space\n",
        "del x_train\n",
        "del y_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "client1_x, client3_x, client1_y, client3y = train_test_split(\n",
        "    client1_x, client1_y, test_size=0.5, random_state=100)\n",
        "print(len(client1_x), len(client2_x), len(client3_x))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Training client 1 model\n",
        "\"\"\"\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "checkpoint_filepath = '/checkpoint/f2-1/'\n",
        "model_checkpointing_callback = ModelCheckpoint(\n",
        "    filepath=checkpoint_filepath,\n",
        "    save_best_only=True,\n",
        ")\n",
        "client1_model.fit(client1_x, client1_y,\n",
        "          batch_size=2048,\n",
        "          epochs=100,\n",
        "          verbose=1,\n",
        "          validation_split=0.1,\n",
        "          callbacks=[callbacks.ReduceLROnPlateau(monitor='loss', patience=10),\n",
        "                     callbacks.EarlyStopping(monitor='loss', patience=15, min_delta=1e-4), model_checkpointing_callback])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RyOYq9mv2ppC"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Training client 2 model\n",
        "\"\"\"\n",
        "checkpoint_filepath = '/checkpoint/f2-2/'\n",
        "model_checkpointing_callback = ModelCheckpoint(\n",
        "    filepath=checkpoint_filepath,\n",
        "    save_best_only=True,\n",
        ")\n",
        "client2_model.fit(client2_x, client2y,\n",
        "          batch_size=2048,\n",
        "          epochs=100,\n",
        "          verbose=1,\n",
        "          validation_split=0.1,\n",
        "          callbacks=[callbacks.ReduceLROnPlateau(monitor='loss', patience=10),\n",
        "                     callbacks.EarlyStopping(monitor='loss', patience=15, min_delta=1e-4), model_checkpointing_callback])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Training client 3 model\n",
        "\"\"\"\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "checkpoint_filepath = '/checkpoint/f2-3/'\n",
        "model_checkpointing_callback = ModelCheckpoint(\n",
        "    filepath=checkpoint_filepath,\n",
        "    save_best_only=True,\n",
        ")\n",
        "client3_model.fit(client3_x, client3y,\n",
        "          batch_size=2048,\n",
        "          epochs=100,\n",
        "          verbose=1,\n",
        "          validation_split=0.1,\n",
        "          callbacks=[callbacks.ReduceLROnPlateau(monitor='loss', patience=10),\n",
        "                     callbacks.EarlyStopping(monitor='loss', patience=15, min_delta=1e-4), model_checkpointing_callback])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Aggregrate models\n",
        "\"\"\"\n",
        "\n",
        "client1_weights = client1_model.get_weights()\n",
        "client2_weights = client2_model.get_weights()\n",
        "client3_weights = client3_model.get_weights()\n",
        "\n",
        "client1_weights = 1/3.35 * client1_weights\n",
        "client2_weights = 1/3.3 * client2_weights\n",
        "client3_weights = 1/3.35 * client3_weights\n",
        "\n",
        "aggregated_weights = numpy.add(client1_weights,client2_weights)\n",
        "aggregated_weights = numpy.add(aggregated_weights, client3_weights)\n",
        "\n",
        "client1_weights.set_weights(aggregated_weights)\n",
        "client1_weights.save_weights(\"model_weight/federated2_weights\", save_format=\"h5\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Chess AI",
      "private_outputs": true,
      "provenance": []
    },
    "interpreter": {
      "hash": "7e7666bfb430d4514d1195031d15ccf80594a969647ade4f9e6074d5049c6fab"
    },
    "kernelspec": {
      "display_name": "Python 3.9.7 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
